{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac02bd2-3072-4fb8-9d09-5c5a3ba1bd3e",
   "metadata": {},
   "source": [
    "### 아래 내용은 글로벌 기업 (주)인텔에서 개발한 <br> Intel® AI for Youth Program 내용을 \n",
    "대한민국 Implementation Partner사인 (주)Brain AI와 Brain AI Lead Coach Network에서 우리나라 초, 중, 고 학생들의 AI 교육을 위해 현지화 한 내용입니다. <br>\n",
    "(주)Brain AI와 NDA를 체결한 학교에서만 사용 가능하며 NDA를 준수해야합니다. <br>\n",
    "상업적 사용은 불가하며, 학교내에서 학생들 교육활동에 자유롭게 사용가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732b498-54fa-4f59-b93a-2497c18d9f40",
   "metadata": {},
   "source": [
    "# 프로젝트 제목: Brain AI 감정 인식 AI 시스템 만들기\n",
    "Intel OpenVINO Pre-trained 모델을 활용하여 감정 인식 AI 시스템 개발 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3bb23-f3b9-4844-afd5-10e036572790",
   "metadata": {},
   "source": [
    "## 1.  Compile Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e2ceccf-0001-47d6-a633-9a2bda99c4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from flask_wtf import FlaskForm\n",
    "from wtforms.fields import FileField, SubmitField\n",
    "from wtforms.validators import InputRequired\n",
    "from openvino.runtime import Core\n",
    "from flask import Flask, render_template, request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2086169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer shape:  [1,3,384,672]\n",
      "Output layer shape: [1,1,200,7]\n"
     ]
    }
   ],
   "source": [
    "ie = Core()\n",
    "\n",
    "model = ie.read_model(model=\"./model/face-detection-adas-0001.xml\")\n",
    "face_model = ie.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "face_input_layer = face_model.input(0)\n",
    "face_output_layer = face_model.output(0)\n",
    "print(\"Input layer shape: \", face_input_layer.shape)\n",
    "print(\"Output layer shape:\",face_output_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c356fe49-25ff-4d63-9a08-7fa013a2c27a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer shape:  [1,3,64,64]\n",
      "Output layer shape: [1,5,1,1]\n"
     ]
    }
   ],
   "source": [
    "model = ie.read_model(model=\"./model/emotions-recognition-retail-0003.xml\")\n",
    "emotion_model = ie.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "emotion_input_layer = model.input(0)\n",
    "emotion_output_layer = emotion_model.output(0)\n",
    "print(\"Input layer shape: \", emotion_input_layer.shape)\n",
    "print(\"Output layer shape:\",emotion_output_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0abbc9-5a4e-496c-b042-88dfc37045da",
   "metadata": {},
   "source": [
    "## 2. 새로운 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20d1ed5a-0c91-4083-b9c0-fd2bbdaa528e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# frame = cv2.imread(\"images/emotions.jpg\")\n",
    "\n",
    "# resized_frame = cv2.resize(src=frame, dsize=(672, 384)) \n",
    "# transposed_frame = resized_frame.transpose(2, 0, 1)\n",
    "# input_frame = np.expand_dims(transposed_frame, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a3e91-f81a-46d9-882c-6fa5a24c83c1",
   "metadata": {},
   "source": [
    "## 3. 추론 및 배포 Inference + Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddab6d09-a4b0-45fd-83e2-a9e2d6aae69f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DrawBoundingBoxes(output, frame, conf=0.5):\n",
    "    boxes = []\n",
    "    frame.copy()                             # 원본 이미지를 수정하는 대신 복사합니다\n",
    "    h,w,_ = frame.shape\n",
    "    \n",
    "    predictions = output[0][0]                         # 하위 집합 데이터 프레임\n",
    "    confidence = predictions[:,2]                       # conf 값 가져오기 [image_id, label, conf, x_min, y_min, x_max, y_max]\n",
    "  \n",
    "    top_predictions = predictions[(confidence>conf)]         # 임계값보다 큰 conf 값을 가진 예측만 선택\n",
    " \n",
    "    for detection in top_predictions:\n",
    "        box = (detection[3:7]* np.array([w, h, w, h])).astype(\"int\") # 상자 위치 결정\n",
    "        (xmin, ymin, xmax, ymax) = box   # xmin, ymin, xmax, ymax에 상자 위치 값 지정\n",
    "        boxes.append(box)\n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)       # 사각형 만들기\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "121a83a8-ff08-49df-986b-bcb50b54b520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DrawText(output, frame, face_position):\n",
    "    \n",
    "    emotions = {\n",
    "        0:\"neutral\",\n",
    "        1:\"happy\",\n",
    "        2:\"sad\",\n",
    "        3:\"surprise\",\n",
    "        4:\"anger\"\n",
    "    }\n",
    "                \n",
    "    predictions = output[0,:,0,0]\n",
    "   \n",
    "    topresult_index = np.argmax(predictions)\n",
    "    emotion = emotions[topresult_index]\n",
    "   \n",
    "    cv2.putText(frame, emotion,\n",
    "                (face_position[0],face_position[1]),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 2, \n",
    "                (255, 255,255), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "708ca67a-0d91-425d-8e20-cd7ab5b6d2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     exit()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m webcam\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[0;32m----> 8\u001b[0m     status, frame \u001b[38;5;241m=\u001b[39m \u001b[43mwebcam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status:\n\u001b[1;32m     11\u001b[0m         resized_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(src\u001b[38;5;241m=\u001b[39mframe, dsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m672\u001b[39m, \u001b[38;5;241m384\u001b[39m)) \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "if not webcam.isOpened():\n",
    "    print(\"Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "while webcam.isOpened():\n",
    "    status, frame = webcam.read()\n",
    "\n",
    "    if status:\n",
    "        resized_frame = cv2.resize(src=frame, dsize=(672, 384)) \n",
    "        transposed_frame = resized_frame.transpose(2, 0, 1)\n",
    "        input_frame = np.expand_dims(transposed_frame, 0)\n",
    "    \n",
    "        face_output = face_model([input_frame])[face_output_layer]\n",
    "        boxes = DrawBoundingBoxes(face_output, frame, conf=0.5)\n",
    "        \n",
    "        if boxes is not None:\n",
    "            \n",
    "            for box in boxes:\n",
    "            \n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                emotion_input = frame[ymin:ymax,xmin:xmax]\n",
    "                if emotion_input.shape[0] < 64 or emotion_input.shape[1] < 64: continue\n",
    "                \n",
    "                resized_image = cv2.resize(src=emotion_input, dsize=(64, 64))\n",
    "                transposed_image = resized_image.transpose(2, 0, 1)\n",
    "                input_image = np.expand_dims(transposed_image, 0)\n",
    "        \n",
    "                emotion_output = emotion_model([input_image])[emotion_output_layer]\n",
    "                DrawText(emotion_output, frame, box)\n",
    "        \n",
    "        cv2.imshow(\"Face\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efc0482e-6b43-483a-a6d4-7a5ba9b430b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [15/Apr/2024 17:39:11] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:39:11] \"\u001b[36mGET /static/styles/index-plus.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:39:11] \"\u001b[36mGET /static/styles/index.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:39:11] \"\u001b[36mGET /static/scripts/index.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:39:11] \"\u001b[35m\u001b[1mGET /static/audio/happy.mp3 HTTP/1.1\u001b[0m\" 206 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:39:11] \"\u001b[35m\u001b[1mGET /static/audio/neutral.mp3 HTTP/1.1\u001b[0m\" 206 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:39:11] \"\u001b[35m\u001b[1mGET /static/audio/sad.mp3 HTTP/1.1\u001b[0m\" 206 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:39:11] \"\u001b[35m\u001b[1mGET /static/audio/surprise.mp3 HTTP/1.1\u001b[0m\" 206 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:39:11] \"\u001b[35m\u001b[1mGET /static/audio/anger.mp3 HTTP/1.1\u001b[0m\" 206 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:40:12] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:40:12] \"\u001b[36mGET /static/scripts/index.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:40:12] \"\u001b[36mGET /static/styles/index-plus.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:40:12] \"\u001b[36mGET /static/styles/index.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:40:12] \"\u001b[35m\u001b[1mGET /static/audio/happy.mp3 HTTP/1.1\u001b[0m\" 206 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:40:12] \"\u001b[35m\u001b[1mGET /static/audio/neutral.mp3 HTTP/1.1\u001b[0m\" 206 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:40:12] \"\u001b[35m\u001b[1mGET /static/audio/sad.mp3 HTTP/1.1\u001b[0m\" 206 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:40:12] \"\u001b[35m\u001b[1mGET /static/audio/surprise.mp3 HTTP/1.1\u001b[0m\" 206 -\n",
      "127.0.0.1 - - [15/Apr/2024 17:40:12] \"\u001b[35m\u001b[1mGET /static/audio/anger.mp3 HTTP/1.1\u001b[0m\" 206 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "app.config[\"SECRET_KEY\"] = \"\"\n",
    "\n",
    "class UploadFileForm(FlaskForm):\n",
    "    file = FileField(\"File\", validators = [ InputRequired() ])\n",
    "    submit = SubmitField(\"Upload File\")\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route(\"/recognize\", methods=[\"POST\"])\n",
    "def recognize():\n",
    "    form = UploadFileForm()\n",
    "    \n",
    "    if form.validate_on_submit():\n",
    "        file = form.file.data\n",
    "        file_type = str(os.path.splitext(file.filename)[1])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
